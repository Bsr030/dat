{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**FIND**-**S**"
      ],
      "metadata": {
        "id": "WmkUue0gUaE0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5PLw9E4Tu8Y"
      },
      "outputs": [],
      "source": [
        "#find s\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "print(data,\"n\")\n",
        "\n",
        "data = np.array(data)[:,:-1]\n",
        "print(\"n The attributes are: \",d)\n",
        "\n",
        "target = np.array(data)[:,-1]\n",
        "print(\"n The target is: \",target)\n",
        "\n",
        "def train(c,t):\n",
        "    for i, val in enumerate(t):\n",
        "        if val == \"Yes\":\n",
        "            specific_hypothesis = c[i].copy()\n",
        "            break\n",
        "\n",
        "    for i, val in enumerate(c):\n",
        "        if t[i] == \"Yes\":\n",
        "            for x in range(len(specific_hypothesis)):\n",
        "                if val[x] != specific_hypothesis[x]:\n",
        "                    specific_hypothesis[x] = '?'\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "    return specific_hypothesis\n",
        "\n",
        "print(\"n The final hypothesis is:\",train(data,target))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN** **algorithm**"
      ],
      "metadata": {
        "id": "G8rPQEyKUNKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#knn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "arr=pd.read_csv('iris.csv')\n",
        "x=arr.drop('Id',axis=\"columns\")\n",
        "a=np.array(x)\n",
        "res={}\n",
        "row=len(a)\n",
        "col=len(a[0])\n",
        "print(\"enter test sample\")\n",
        "test=[]\n",
        "for i in range(col-1):\n",
        "    test.append(float(input()))\n",
        "for i in range(row):\n",
        "    dist=0\n",
        "    for j in range(col-1):\n",
        "        dist=dist+(test[j]-a[i][j])**2\n",
        "    res[dist]=a[i][col-1]\n",
        "res=dict(sorted(res.items()))\n",
        "print(\"enter k value\")\n",
        "k=int(input())\n",
        "result=list(res.items())[:k]\n",
        "print(result)\n",
        "setosa=0\n",
        "vercol=0\n",
        "vir=0\n",
        "for i in range(k):\n",
        "    if result[i][1]=='Iris-setosa':\n",
        "        setosa+=1\n",
        "    elif result[i][1]=='Iris-versicolor':\n",
        "        vercol+=1\n",
        "    elif result[i][1]=='Iris-virginica':\n",
        "        vir+=1\n",
        "if max(setosa,vercol,vir)==setosa:\n",
        " print(\"setosa\")\n",
        "elif max(setosa,vercol,vir)==vercol:\n",
        " print(\"versicolor\")\n",
        "elif max(setosa,vercol,vir)==vir:\n",
        " print(\"virginia\")"
      ],
      "metadata": {
        "id": "Ds5-6GDfUF4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOCALLY** **WEIGHTED** **REGRESSION**"
      ],
      "metadata": {
        "id": "yQagWJWuc586"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#locally weighted regression\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('tips.csv')\n",
        "features = np.array(df.total_bill)\n",
        "labels = np.array(df.tip)\n",
        "\n",
        "def kernel(data, point, xmat, k):\n",
        "   m,n = np.shape(xmat)\n",
        "   ws = np.mat(np.eye((m)))\n",
        "   for j in range(m):\n",
        "      diff = point - data[j]\n",
        "      ws[j,j] = np.exp(diff*diff.T/(-2.0*k**2))\n",
        "   return ws\n",
        "\n",
        "def local_weight(data, point, xmat, ymat, k):\n",
        "   wei = kernel(data, point, xmat, k)\n",
        "   return (data.T*(wei*data)).I*(data.T*(wei*ymat.T))\n",
        "\n",
        "def local_weight_regression(xmat, ymat, k):\n",
        "   m,n = np.shape(xmat)\n",
        "   ypred = np.zeros(m)\n",
        "   for i in range(m):\n",
        "      ypred[i] = xmat[i]*local_weight(xmat, xmat[i],xmat,ymat,k)\n",
        "   return ypred\n",
        "\n",
        "m = features.shape[0]\n",
        "mtip = np.mat(labels)\n",
        "data = np.hstack((np.ones((m, 1)), np.mat(features).T))\n",
        "\n",
        "ypred = local_weight_regression(data, mtip, 0.5)\n",
        "indices = data[:,1].argsort(0)\n",
        "xsort = data[indices][:,0]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.scatter(features, labels, color='blue')\n",
        "ax.plot(xsort[:,1],ypred[indices], color = 'red', linewidth=3)\n",
        "plt.xlabel('Total bill')\n",
        "plt.ylabel('Tip')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RvuheyLGUKn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DECISION** **TREE**"
      ],
      "metadata": {
        "id": "cuMkz6I6QvfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import tree\n",
        "\n",
        "df = pd.read_csv('salaries.csv')\n",
        "df.head()\n",
        "inputs = df.drop('salary_more_then_100k',axis = 'columns')\n",
        "target = df['salary_more_then_100k']\n",
        "\n",
        "le_company = LabelEncoder()\n",
        "le_job = LabelEncoder()\n",
        "le_degree = LabelEncoder()\n",
        "inputs['company_n'] = le_company.fit_transform(inputs['company'])\n",
        "inputs['job_n'] = le_job.fit_transform(inputs['job'])\n",
        "inputs['degree_n'] = le_degree.fit_transform(inputs['degree'])\n",
        "inputs.head()\n",
        "inputs_n = inputs.drop(['company','job','degree'],axis='columns')\n",
        "inputs_n\n",
        "\n",
        "model = tree.DecisionTreeClassifier()\n",
        "model.fit(inputs_n,target)\n",
        "model.score(inputs_n,target)\n",
        "model.predict([[2,1,0]])\n",
        "model.predict([[2,1,1]])"
      ],
      "metadata": {
        "id": "PXAaw4FiQ2UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PERCEPTRON** **XOR**"
      ],
      "metadata": {
        "id": "ggmlViF-Q4ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def unitStep(v):\n",
        "    if v >= 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def perceptronModel(x, w, b):\n",
        "    v = np.dot(w, x) + b\n",
        "    y = unitStep(v)\n",
        "    return y\n",
        "\n",
        "def NOT_logicFunction(x):\n",
        "    wNOT = -1\n",
        "    bNOT = 0.5\n",
        "    return perceptronModel(x, wNOT, bNOT)\n",
        "\n",
        "def AND_logicFunction(x):\n",
        "    w = np.array([1, 1])\n",
        "    bAND = -1.5\n",
        "    return perceptronModel(x, w, bAND)\n",
        "\n",
        "def OR_logicFunction(x):\n",
        "    w = np.array([1, 1])\n",
        "    bOR = -0.5\n",
        "    return perceptronModel(x, w, bOR)\n",
        "\n",
        "def XOR_logicFunction(x):\n",
        "    y1 = AND_logicFunction(x)\n",
        "    y2 = OR_logicFunction(x)\n",
        "    y3 = NOT_logicFunction(y1)\n",
        "    final_x = np.array([y2, y3])\n",
        "    finalOutput = AND_logicFunction(final_x)\n",
        "    return finalOutput\n",
        "\n",
        "test1 = np.array([0, 1])\n",
        "test2 = np.array([1, 1])\n",
        "test3 = np.array([0, 0])\n",
        "test4 = np.array([1, 0])\n",
        "print(\"XOR({}, {}) = {}\".format(0, 1, XOR_logicFunction(test1)))\n",
        "print(\"XOR({}, {}) = {}\".format(1, 1, XOR_logicFunction(test2)))\n",
        "print(\"XOR({}, {}) = {}\".format(0, 0, XOR_logicFunction(test3)))\n",
        "print(\"XOR({}, {}) = {}\".format(1, 0, XOR_logicFunction(test4)))"
      ],
      "metadata": {
        "id": "NOrNkfOHQ9wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SINGLE** **LAYER** **PERCEPTRON**"
      ],
      "metadata": {
        "id": "GgV1Dr-gRTA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math as math\n",
        "import numpy\n",
        "def actiF(t):\n",
        "    return 1/(1+math.exp(-t))\n",
        "w=[1,2,3,2]\n",
        "x=[1,0,1,0]\n",
        "b=0.5\n",
        "n=1\n",
        "target = numpy.bitwise_xor.reduce(x)\n",
        "print(target)\n",
        "def summ(w,x):\n",
        "    t=0\n",
        "    for i in range(len(w)):\n",
        "        t+=w[i]*x[i]\n",
        "    return t\n",
        "y=actiF(summ(w,x))\n",
        "print(y)\n",
        "def er(target,y):\n",
        "    error=target-y\n",
        "    return error\n",
        "print(er(target,y))\n",
        "def weiup(w,error):\n",
        "    for i in range(len(w)):\n",
        "        w[i]=w[i]+n*error*x[i]\n",
        "    return w\n",
        "for i in range(5):\n",
        "    p=summ(w,x)\n",
        "    q=actiF(p)\n",
        "    r=er(target,q)\n",
        "    if r!=0:\n",
        "        w=weiup(w,r)\n",
        "    elif r==0:\n",
        "        break\n",
        "print(w)\n",
        "print(target)\n",
        "print(q)\n"
      ],
      "metadata": {
        "id": "Phzf5dTgReH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MULTILAYER** **NETWORK**"
      ],
      "metadata": {
        "id": "sXaSVxkk9IB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MULTI LAYER PERCEPTRON TRAINING RULE\n",
        "\n",
        "import math\n",
        "import random\n",
        "\n",
        "a = 0.01\n",
        "\n",
        "def activation(y):\n",
        "    s = 1 / (1 + math.exp(-y))\n",
        "    return s\n",
        "\n",
        "def updateWeights(x, e, w):\n",
        "    r = [wi - a * e * xi for xi, wi in zip(x, w)]\n",
        "    return r\n",
        "\n",
        "print(\"Enter the no.of input nodes:\")\n",
        "i = int(input())\n",
        "print(\"Enter the no.of nodes in hidden layers:\")\n",
        "h = int(input())\n",
        "print(\"Enter the no.of output nodes:\")\n",
        "o = int(input())\n",
        "\n",
        "x = [0] * i\n",
        "w = [random.uniform(0.0, 1.0) for _ in range(i * h)]\n",
        "v = [random.uniform(0.0, 1.0) for _ in range(h * o)]\n",
        "\n",
        "for j in range(i):\n",
        "    x[j] = int(input(\"Enter input for node {}: \".format(j)))\n",
        "\n",
        "p = [0] * h\n",
        "q = [0] * o  # Output layer values\n",
        "\n",
        "# Forward pass through the hidden layer\n",
        "for k in range(h):\n",
        "    for j in range(i):\n",
        "        p[k] += w[k * i + j] * x[j]\n",
        "\n",
        "# Forward pass through the output layer\n",
        "for k in range(o):\n",
        "    for j in range(h):\n",
        "        q[k] += v[k * h + j] * activation(p[j])\n",
        "\n",
        "print(\"Hidden layer outputs:\", p)\n",
        "print(\"Output layer outputs:\", q)"
      ],
      "metadata": {
        "id": "pVLxrT0R9HZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K** **MEANS**"
      ],
      "metadata": {
        "id": "UhpmHvP2RewG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "df = pd.read_csv('iris.csv')\n",
        "\n",
        "def initialize_centroids(data, k):\n",
        "  np.random.seed(0)\n",
        "  centroids = data.sample(n=k)\n",
        "  return centroids.iloc[:, :-1].values\n",
        "\n",
        "def euclidean_distance(point1, point2):\n",
        "  return math.sqrt(sum((p1 - p2) ** 2 for p1, p2 in zip(point1, point2)))\n",
        "\n",
        "def assign_to_clusters(data, centroids, k):\n",
        "    clusters = [[] for _ in range(k)]\n",
        "    for index, row in data.iterrows():\n",
        "        point = row[:-1].values\n",
        "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
        "        closest_cluster = distances.index(min(distances))\n",
        "        clusters[closest_cluster].append(point)\n",
        "    return clusters\n",
        "\n",
        "def update_centroids(clusters):\n",
        "    new_centroids = []\n",
        "    for cluster in clusters:\n",
        "        if cluster:\n",
        "            new_centroid = np.mean(cluster, axis=0)\n",
        "            new_centroids.append(new_centroid)\n",
        "    return np.array(new_centroids)\n",
        "\n",
        "def has_converged(centroids, new_centroids, iteration, max_iterations):\n",
        "    if iteration >= max_iterations:\n",
        "        return True\n",
        "    return np.array_equal(centroids, new_centroids)\n",
        "\n",
        "def k_means(data, k, max_iterations):\n",
        "    centroids = initialize_centroids(data, k)\n",
        "    iteration = 0\n",
        "\n",
        "    while True:\n",
        "        clusters = assign_to_clusters(data, centroids, k)\n",
        "        new_centroids = update_centroids(clusters)\n",
        "\n",
        "\n",
        "\n",
        "        if has_converged(centroids, new_centroids, iteration, max_iterations):\n",
        "            break\n",
        "\n",
        "        centroids = new_centroids\n",
        "        iteration += 1\n",
        "\n",
        "    return centroids, clusters\n",
        "\n",
        "def plot_clusters(centroids, clusters):\n",
        "    colors = ['r', 'g', 'b']\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        cluster = np.array(cluster)\n",
        "        plt.scatter(cluster[:, 0], cluster[:, 1], c=colors[i], label=f'Cluster {i + 1}')\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1], s=100, c='black', marker='X', label='Centroids')\n",
        "    plt.xlabel('Sepal Length')\n",
        "    plt.ylabel('Sepal Width')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "centroids, clusters = k_means(df, 3, 100)\n",
        "plot_clusters(centroids, clusters)\n"
      ],
      "metadata": {
        "id": "UmEp-MjORkT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EM** **ALGO**"
      ],
      "metadata": {
        "id": "0ufXyHe5RptB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_iris_data():\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "    column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
        "    data = pd.read_csv(url, names=column_names)\n",
        "    return data\n",
        "\n",
        "def initialize_parameters(data, k):\n",
        "    n_samples, n_features = data.shape\n",
        "    np.random.seed(0)\n",
        "\n",
        "    means = data.sample(n=k).values[:, :-1]\n",
        "\n",
        "    covariances = [np.identity(n_features - 1) * 1e-3 for _ in range(k)]\n",
        "\n",
        "    mixing_coefficients = [1.0 / k] * k\n",
        "\n",
        "    return means, covariances, mixing_coefficients\n",
        "\n",
        "def calculate_probabilities(data, means, covariances, mixing_coefficients):\n",
        "    n_samples, _ = data.shape\n",
        "    k = len(means)\n",
        "    probabilities = np.zeros((n_samples, k))\n",
        "\n",
        "    for i in range(k):\n",
        "        for j in range(n_samples):\n",
        "            diff = data.iloc[j, :-1].values - means[i]\n",
        "            inv_covariance = np.linalg.inv(covariances[i])\n",
        "            exponent = -0.5 * np.dot(np.dot(diff, inv_covariance), diff)\n",
        "            prob = math.exp(exponent) / (2 * math.pi * np.sqrt(np.linalg.det(covariances[i])))\n",
        "            probabilities[j, i] = mixing_coefficients[i] * prob\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "def update_parameters(data, probabilities):\n",
        "    n_samples, _ = data.shape\n",
        "    k = probabilities.shape[1]\n",
        "\n",
        "    means = np.dot(probabilities.T, data.iloc[:, :-1].values) / np.sum(probabilities, axis=0)[:, np.newaxis]\n",
        "\n",
        "    covariances = []\n",
        "    for i in range(k):\n",
        "        diff = data.iloc[:, :-1].values - means[i]\n",
        "        weighted_diff = (diff.T * probabilities[:, i]).T\n",
        "        cov = np.dot(weighted_diff.T, diff) / np.sum(probabilities[:, i])\n",
        "        covariances.append(cov)\n",
        "\n",
        "    mixing_coefficients = np.mean(probabilities, axis=0) / n_samples\n",
        "\n",
        "    return means, covariances, mixing_coefficients\n",
        "\n",
        "def has_converged(means, new_means, tolerance=1e-4):\n",
        "    return np.all(np.abs(means - new_means) < tolerance)\n",
        "\n",
        "def em_algorithm(data, k, max_iterations):\n",
        "    means, covariances, mixing_coefficients = initialize_parameters(data, k)\n",
        "    iteration = 0\n",
        "\n",
        "    while iteration < max_iterations:\n",
        "        probabilities = calculate_probabilities(data, means, covariances, mixing_coefficients)\n",
        "        new_means, new_covariances, new_mixing_coefficients = update_parameters(data, probabilities)\n",
        "\n",
        "        if has_converged(means, new_means):\n",
        "            break\n",
        "\n",
        "        means, covariances, mixing_coefficients = new_means, new_covariances, new_mixing_coefficients\n",
        "        iteration += 1\n",
        "\n",
        "    return means, covariances, mixing_coefficients\n",
        "\n",
        "def plot_clusters(data, means):\n",
        "    plt.scatter(data['sepal_length'], data['sepal_width'], c='b', label='Data Points')\n",
        "    plt.scatter(means[:, 0], means[:, 1], s=100, c='r', marker='X', label='Cluster Centers')\n",
        "    plt.xlabel('Sepal Length')\n",
        "    plt.ylabel('Sepal Width')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "iris_data = load_iris_data()\n",
        "k = 3\n",
        "max_iterations = 100\n",
        "means, _, _ = em_algorithm(iris_data, k, max_iterations)\n",
        "plot_clusters(iris_data, means)"
      ],
      "metadata": {
        "id": "BYkZnxEfR_WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNINFORMED** **SEARCH**"
      ],
      "metadata": {
        "id": "T8jZ6W28SFIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**BFS**"
      ],
      "metadata": {
        "id": "TFsdg9bASPMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = {\n",
        "  'A' : ['B','C'],\n",
        "  'B' : ['D', 'E'],\n",
        "  'C' : ['F', 'G'],\n",
        "  'D' : [],\n",
        "  'E' : [],\n",
        "  'F' : [],\n",
        "  'G' : []\n",
        "}\n",
        "\n",
        "visited = []\n",
        "queue = []\n",
        "\n",
        "def bfs(visited, graph, node):\n",
        "  queue.append(node)\n",
        "  print(\"Frontier: \",end=\" \")\n",
        "  print(queue)\n",
        "  visited.append(node)\n",
        "  print(\"Explored:\",end=\" \")\n",
        "  print(visited)\n",
        "\n",
        "\n",
        "  while queue:\n",
        "    m = queue.pop(0)\n",
        "    print(m)\n",
        "\n",
        "    for neighbour in graph[m]:\n",
        "      if neighbour not in visited:\n",
        "        queue.append(neighbour)\n",
        "        print(\"Frontier: \",end=\" \")\n",
        "        print(queue)\n",
        "        visited.append(neighbour)\n",
        "        print(\"Explored:\",end=\" \")\n",
        "        print(visited)\n",
        "\n",
        "\n",
        "print(\"Following is the Breadth-First Search\")\n",
        "bfs(visited, graph, 'A')\n"
      ],
      "metadata": {
        "id": "f1Hpd4A6SOeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DFS**"
      ],
      "metadata": {
        "id": "60gmxFfLSlIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = {\n",
        "    'A' : ['B','C'],\n",
        "    'B' : ['D', 'E'],\n",
        "    'C' : ['F', 'G'],\n",
        "    'D' : [],\n",
        "    'E' : [],\n",
        "    'F' : [],\n",
        "    'G' : []\n",
        "}\n",
        "\n",
        "\n",
        "visited = []\n",
        "frontier=[]\n",
        "def dfs(visited, graph, node):\n",
        "    if node not in visited:\n",
        "        print (node)\n",
        "        visited.append(node)\n",
        "        print(\"explored: \" ,end=\" \")\n",
        "        print(visited)\n",
        "        for neighbour in graph[node]:\n",
        "          frontier.insert(len(visited)-1,neighbour)\n",
        "          print(\"frontier:\" ,end=\" \")\n",
        "          print(frontier)\n",
        "          dfs(visited, graph, neighbour)\n",
        "\n",
        "\n",
        "dfs(visited, graph, 'A')"
      ],
      "metadata": {
        "id": "TXw4Th2WSm_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DLS**"
      ],
      "metadata": {
        "id": "vyDTrUOQSsGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = {\n",
        "    'A' : ['B','C'],\n",
        "    'B' : ['D', 'E'],\n",
        "    'C' : ['F', 'G'],\n",
        "    'D' : ['H','I'],\n",
        "    'E' : ['J','K'],\n",
        "    'F' : [],\n",
        "    'G' : []\n",
        "}\n",
        "\n",
        "depth = {'A' : 0, 'B' : 1,'C' : 1,'D' : 2,'E' : 2,'F' : 2,'G' : 2 ,'H':3, 'I':3, 'J':3 , 'K':3}\n",
        "depth_limit =2\n",
        "visited = []\n",
        "frontier=[]\n",
        "def dls(visited, graph, node):\n",
        "    if node not in visited and depth[node]<=depth_limit:\n",
        "        print (node)\n",
        "        visited.append(node)\n",
        "        # print(\"explored: \" ,end=\" \")\n",
        "        # print(visited)\n",
        "        for neighbour in graph[node]:\n",
        "          frontier.insert(len(visited)-1,neighbour)\n",
        "          # print(\"frontier:\" ,end=\" \")\n",
        "          # print(frontier)\n",
        "          dls(visited, graph, neighbour)\n",
        "\n",
        "\n",
        "dls(visited, graph, 'A')"
      ],
      "metadata": {
        "id": "nhTdRRy0StzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IDDFS**"
      ],
      "metadata": {
        "id": "pBqWVqEFS1xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#iddfs\n",
        "import sys\n",
        "\n",
        "graph = {\n",
        "    'A' : ['B','C'],\n",
        "    'B' : ['D', 'E'],\n",
        "    'C' : ['F', 'G'],\n",
        "    'D' : ['H','I'],\n",
        "    'E' : ['J','K'],\n",
        "    'F' : [],\n",
        "    'G' : [],\n",
        "    'H' : [] ,\n",
        "    'I' : [],\n",
        "    'J': [],\n",
        "    'K': []}\n",
        "limit=0\n",
        "stop=True\n",
        "search= 'H'\n",
        "depth = {'A' : 0, 'B' : 1,'C' : 1,'D' : 2,'E' : 2,'F' : 2,'G' : 2 ,'H':3, 'I':3, 'J':3 , 'K':3}\n",
        "\n",
        "\n",
        "def dls(visited, graph, node,limit):\n",
        "    if node not in visited and depth[node]<=limit:\n",
        "        print(node)\n",
        "        visited.append(node)\n",
        "        if node == search :\n",
        "          stop=False\n",
        "          sys.exit(0)\n",
        "        # print(\"explored: \" ,end=\" \")\n",
        "        # print(visited)\n",
        "        for neighbour in graph[node]:\n",
        "          if depth[neighbour]<=limit:\n",
        "            frontier.insert(len(visited)-1,neighbour)\n",
        "            # print(\"frontier:\" ,end=\" \")\n",
        "            # print(frontier)\n",
        "            dls(visited, graph, neighbour,limit)\n",
        "\n",
        "while stop:\n",
        "  visited = []\n",
        "  frontier=[]\n",
        "  print(\"\\n\")\n",
        "  dls(visited,graph,'A',limit)\n",
        "  limit=limit+1"
      ],
      "metadata": {
        "id": "al5VcvVoS4zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**k** **means** **&** **EM**"
      ],
      "metadata": {
        "id": "ZkfZbLQP2b1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn import preprocessing\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.datasets import load_iris\n",
        "import sklearn.metrics as sm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "dataset=load_iris()\n",
        "X=pd.DataFrame(dataset.data)\n",
        "X.columns=['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\n",
        "y=pd.DataFrame(dataset.target)\n",
        "y.columns=['Targets']\n",
        "plt.figure(figsize=(14,7))\n",
        "colormap=np.array(['gold','purple','red'])\n",
        "\n",
        "# REAL PLOT\n",
        "plt.subplot(1,3,1)\n",
        "plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[y.Targets],s=40)\n",
        "plt.title('Real')\n",
        "\n",
        "# GMM PLOT\n",
        "scaler=preprocessing.StandardScaler()\n",
        "scaler.fit(X)\n",
        "xsa=scaler.transform(X)\n",
        "xs=pd.DataFrame(xsa,columns=X.columns)\n",
        "gmm=GaussianMixture(n_components=3)\n",
        "gmm.fit(xs)\n",
        "y_cluster_gmm=gmm.predict(xs)\n",
        "plt.subplot(1,3,3)\n",
        "plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[y_cluster_gmm],s=40)\n",
        "plt.title('EM Classification')\n",
        "\n",
        "# K-PLOT\n",
        "plt.subplot(1,3,2)\n",
        "model=KMeans(n_clusters=3)\n",
        "model.fit(X)\n",
        "predY=np.choose(model.labels_,[0,1,2]).astype(np.int64)\n",
        "plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[predY],s=40)\n",
        "plt.title('KMeans')"
      ],
      "metadata": {
        "id": "OJ4A7YCY2i45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Star"
      ],
      "metadata": {
        "id": "74ypBxwAPYV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from heapq import heappush, heappop\n",
        "\n",
        "n = 3\n",
        "\n",
        "rows = [ 1, 0, -1, 0 ]\n",
        "cols = [ 0, -1, 0, 1 ]\n",
        "\n",
        "class priorityQueue:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.heap = []\n",
        "\n",
        "    def push(self, key):\n",
        "        heappush(self.heap, key)\n",
        "\n",
        "    def pop(self):\n",
        "        return heappop(self.heap)\n",
        "\n",
        "    def empty(self):\n",
        "        if not self.heap:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "class nodes:\n",
        "\n",
        "    def __init__(self, parent, mats, empty_tile_posi,\n",
        "                costs, levels):\n",
        "\n",
        "        self.parent = parent\n",
        "\n",
        "        self.mats = mats\n",
        "\n",
        "        self.empty_tile_posi = empty_tile_posi\n",
        "\n",
        "        self.costs = costs\n",
        "\n",
        "        self.levels = levels\n",
        "\n",
        "    def __lt__(self, nxt):\n",
        "        return self.costs < nxt.costs\n",
        "\n",
        "def calculateCosts(mats, final) -> int:\n",
        "\n",
        "    count = 0\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if ((mats[i][j]) and\n",
        "                (mats[i][j] != final[i][j])):\n",
        "                count += 1\n",
        "\n",
        "    return count\n",
        "\n",
        "def newNodes(mats, empty_tile_posi, new_empty_tile_posi,\n",
        "            levels, parent, final) -> nodes:\n",
        "\n",
        "    new_mats = copy.deepcopy(mats)\n",
        "\n",
        "    x1 = empty_tile_posi[0]\n",
        "    y1 = empty_tile_posi[1]\n",
        "    x2 = new_empty_tile_posi[0]\n",
        "    y2 = new_empty_tile_posi[1]\n",
        "    new_mats[x1][y1], new_mats[x2][y2] = new_mats[x2][y2], new_mats[x1][y1]\n",
        "\n",
        "    costs = calculateCosts(new_mats, final)\n",
        "\n",
        "    new_nodes = nodes(parent, new_mats, new_empty_tile_posi,\n",
        "                    costs, levels)\n",
        "    return new_nodes\n",
        "\n",
        "def printMatsrix(mats):\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            print(\"%d \" % (mats[i][j]), end = \" \")\n",
        "\n",
        "        print()\n",
        "\n",
        "def isSafe(x, y):\n",
        "\n",
        "    return x >= 0 and x < n and y >= 0 and y < n\n",
        "\n",
        "def printPath(root):\n",
        "\n",
        "    if root == None:\n",
        "        return\n",
        "\n",
        "    printPath(root.parent)\n",
        "    printMatsrix(root.mats)\n",
        "    print()\n",
        "\n",
        "def solve(initial, empty_tile_posi, final):\n",
        "\n",
        "    pq = priorityQueue()\n",
        "\n",
        "    costs = calculateCosts(initial, final)\n",
        "    root = nodes(None, initial,\n",
        "                empty_tile_posi, costs, 0)\n",
        "\n",
        "    pq.push(root)\n",
        "\n",
        "    while not pq.empty():\n",
        "\n",
        "        minimum = pq.pop()\n",
        "\n",
        "        if minimum.costs == 0:\n",
        "\n",
        "            printPath(minimum)\n",
        "            return\n",
        "\n",
        "        for i in range(n):\n",
        "            new_tile_posi = [\n",
        "                minimum.empty_tile_posi[0] + rows[i],\n",
        "                minimum.empty_tile_posi[1] + cols[i], ]\n",
        "\n",
        "            if isSafe(new_tile_posi[0], new_tile_posi[1]):\n",
        "\n",
        "                child = newNodes(minimum.mats,\n",
        "                                minimum.empty_tile_posi,\n",
        "                                new_tile_posi,\n",
        "                                minimum.levels + 1,\n",
        "                                minimum, final,)\n",
        "\n",
        "                pq.push(child)\n",
        "\n",
        "initial = [ [ 1, 2, 3 ],\n",
        "            [ 5, 6, 0 ],\n",
        "            [ 7, 8, 4 ] ]\n",
        "\n",
        "final = [ [ 1, 2, 3 ],\n",
        "        [ 5, 8, 6 ],\n",
        "        [ 0, 7, 4 ] ]\n",
        "\n",
        "empty_tile_posi = [ 1, 2 ]\n",
        "\n",
        "solve(initial, empty_tile_posi, final)"
      ],
      "metadata": {
        "id": "nXy9QuIvPaYs",
        "outputId": "1cf118ff-abc9-427b-cc89-d389137b8973",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1  2  3  \n",
            "5  6  0  \n",
            "7  8  4  \n",
            "\n",
            "1  2  3  \n",
            "5  0  6  \n",
            "7  8  4  \n",
            "\n",
            "1  2  3  \n",
            "5  8  6  \n",
            "7  0  4  \n",
            "\n",
            "1  2  3  \n",
            "5  8  6  \n",
            "0  7  4  \n",
            "\n"
          ]
        }
      ]
    }
  ]
}